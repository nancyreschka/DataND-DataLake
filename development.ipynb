{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "#input_data = \"s3a://udacity-dend/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "#song_data_path = f'{input_data}/song_data/A/A/*/*.json'\n",
    "song_data_path = f'/home/workspace/data/song_data/*/*/*/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, DateType, LongType, TimestampType\n",
    "songdataSchema = StructType([\n",
    "    StructField(\"num_songs\", IntegerType()),\n",
    "    StructField(\"artist_id\", StringType()),\n",
    "    StructField(\"artist_latitude\", DoubleType()),\n",
    "    StructField(\"artist_longitude\", DoubleType()),\n",
    "    StructField(\"artist_location\", StringType()),\n",
    "    StructField(\"artist_name\", StringType()),\n",
    "    StructField(\"song_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"duration\", DoubleType()),\n",
    "    StructField(\"year\", IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read song data file\n",
    "song_df = spark.read.json(song_data_path, songdataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_df.printSchema()\n",
    "song_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "song_df.createOrReplaceTempView(\"song_data\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM song_data\n",
    "\"\"\")\n",
    "songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "num = spark.sql(\"\"\"\n",
    "    SELECT count(song_id)\n",
    "    FROM song_data\n",
    "\"\"\")\n",
    "num.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#output_data = \"s3a://nancys-data-lake-project/\"\n",
    "output_data = \"/home/workspace/result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.parquet(os.path.join(output_data, \"songs/\"), mode='overwrite', partitionBy=[\"year\",\"artist_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT artist_id, artist_name AS name, artist_location AS location, artist_latitude AS latitude, \n",
    "    artist_longitude AS longitude\n",
    "    FROM song_data\n",
    "\"\"\")\n",
    "artists_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.parquet(os.path.join(output_data, \"artists/\"), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "#log_data = '{input_data}/log_data/'\n",
    "#log_data = f'{input_data}/log_data/2018/11/2018-11-08-events.json'\n",
    "log_data_path = f'/home/workspace/data/log_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logdataSchema = StructType([\n",
    "    StructField(\"artist\", StringType()),\n",
    "    StructField(\"auth\", StringType()),\n",
    "    StructField(\"firstName\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"itemInSession\", IntegerType()),\n",
    "    StructField(\"lastName\", StringType()),\n",
    "    StructField(\"length\", DoubleType()),\n",
    "    StructField(\"level\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"method\", StringType()),\n",
    "    StructField(\"page\", StringType()),\n",
    "    StructField(\"registration\", DoubleType()),\n",
    "    StructField(\"sessionId\", IntegerType()),\n",
    "    StructField(\"song\", StringType()),\n",
    "    StructField(\"status\", IntegerType()),\n",
    "    StructField(\"ts\", LongType()),\n",
    "    StructField(\"userAgent\", StringType()),\n",
    "    StructField(\"userId\", StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read log data file\n",
    "logdf = spark.read.json(log_data_path, logdataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logdf.printSchema()\n",
    "logdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "logdf = logdf.where(\"page == 'NextSong'\")\n",
    "logdf.createOrReplaceTempView(\"log_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tempdf = spark.sql(\"\"\"\n",
    "    SELECT max(s2.ts)\n",
    "    FROM log_data s2 \n",
    "\"\"\")\n",
    "tempdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns for users table    \n",
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT s1.userId AS user_id, \n",
    "        s1.firstName AS first_name, \n",
    "        s1.lastName AS last_name, \n",
    "        s1.gender, \n",
    "        s1.level\n",
    "    FROM log_data s1\n",
    "    WHERE s1.userId IS NOT NULL \n",
    "    AND s1.ts = (SELECT max(s2.ts)\n",
    "                        FROM log_data s2\n",
    "                        WHERE s1.userId = s2.userId)    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.parquet(os.path.join(output_data, \"users/\"), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000.0), TimestampType())\n",
    "logdf = logdf.withColumn('start_time', get_timestamp('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x / 1000.0).strftime('%Y-%m-%d %H:%M:%S'), StringType())\n",
    "logdf = logdf.withColumn('date_time', get_datetime('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "logdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "logdf.createOrReplaceTempView(\"log_data\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT date_time AS start_time,\n",
    "        hour(start_time) AS hour, \n",
    "        day(start_time) AS day, \n",
    "        weekofyear(start_time) AS week, \n",
    "        month(start_time) AS month, \n",
    "        year(start_time) AS year, \n",
    "        weekday(start_time) AS weekday \n",
    "    FROM log_data\n",
    "\"\"\")\n",
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.parquet(os.path.join(output_data, \"time/\"), mode='overwrite', partitionBy=[\"year\",\"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_df = song_df.join(logdf, (song_df.title == logdf.song) & (song_df.artist_name == logdf.artist) & ((song_df.duration == logdf.length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_df = songplays_df.withColumn('songplay_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_df.printSchema()\n",
    "songplays_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_df.createOrReplaceTempView(\"songplays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table  = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        songplay_id,\n",
    "        date_time AS start_time, \n",
    "        userId AS user_id, \n",
    "        level, \n",
    "        song_id, \n",
    "        artist_id, \n",
    "        sessionId AS session_id, \n",
    "        location, \n",
    "        userAgent AS user_agent,\n",
    "        month(start_time) AS month, \n",
    "        year(start_time) AS year\n",
    "    FROM songplays\n",
    "    WHERE userId IS NOT NULL\n",
    "        AND level IS NOT NULL\n",
    "        AND sessionId IS NOT NULL\n",
    "        AND location IS NOT NULL\n",
    "        AND userAgent IS NOT NULL                                \n",
    "        AND date_time IS NOT NULL\n",
    "        AND song_id IS NOT NULL\n",
    "        AND artist_id IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.parquet(os.path.join(output_data, \"songplays/\"), mode='overwrite', partitionBy=[\"year\",\"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
